{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba30e81b",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Text Pre-processing \n",
    "\n",
    "We will perform some common data cleaning steps on all text. Then, we will perform more cleaning after the text has been tokenized.   \n",
    "\n",
    "Data cleaning process can go forever. However, we will start simple and iterate. We can execute the common cleaning steps and inspect our results. If needed, more cleaning can be done at a later point to improve our results. \n",
    "\n",
    "**Common data cleaning steps on all text :**\n",
    "* make text lowercase\n",
    "* remove punctuation\n",
    "* remove numerical values \n",
    "* remove common non-sensical text (\\n) \n",
    "\n",
    "**Word Tokenization :**  \n",
    "Split a sentence into list of words. \n",
    "\n",
    "**More data cleaning steps after tokenization :**\n",
    "* remove stop words\n",
    "* lemmatization for meaning root word\n",
    "* parts of speech tagging \n",
    "* create bi-grams or tri-grams \n",
    "* deal with typos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44801c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pickle\n",
    "import re\n",
    "import string \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abd614b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comedian</th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Subtitle</th>\n",
       "      <th>Transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chris Rock</td>\n",
       "      <td>March 8, 2023</td>\n",
       "      <td>Selective Outrage (2023) | Transcript</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[slow instrumental music playing] [funk drums ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marc Maron</td>\n",
       "      <td>March 3, 2023</td>\n",
       "      <td>Thinky Pain (2013) | Transcript</td>\n",
       "      <td>Marc Maron returns to his old stomping grounds...</td>\n",
       "      <td>[siren wailing] I don’t know what you were thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chelsea Handler</td>\n",
       "      <td>March 3, 2023</td>\n",
       "      <td>Evolution (2020) | Transcript</td>\n",
       "      <td>Chelsea Handler is back and better than ever -...</td>\n",
       "      <td>Join me in welcoming the author of six number ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tom Papa</td>\n",
       "      <td>March 3, 2023</td>\n",
       "      <td>What A Day! (2022) | Transcript</td>\n",
       "      <td>Follows Papa as he shares about parenting, his...</td>\n",
       "      <td>Premiered on December 13, 2022 Ladies and gent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jim Jefferies</td>\n",
       "      <td>February 22, 2023</td>\n",
       "      <td>High n’ Dry (2023) | Transcript</td>\n",
       "      <td>Jim Jefferies is back and no topic is off limi...</td>\n",
       "      <td>Please welcome to the stage, Jim Jefferies! He...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Comedian               Date                                  Title  \\\n",
       "0       Chris Rock      March 8, 2023  Selective Outrage (2023) | Transcript   \n",
       "1       Marc Maron      March 3, 2023        Thinky Pain (2013) | Transcript   \n",
       "2  Chelsea Handler      March 3, 2023          Evolution (2020) | Transcript   \n",
       "3         Tom Papa      March 3, 2023        What A Day! (2022) | Transcript   \n",
       "4    Jim Jefferies  February 22, 2023        High n’ Dry (2023) | Transcript   \n",
       "\n",
       "                                            Subtitle  \\\n",
       "0                                                NaN   \n",
       "1  Marc Maron returns to his old stomping grounds...   \n",
       "2  Chelsea Handler is back and better than ever -...   \n",
       "3  Follows Papa as he shares about parenting, his...   \n",
       "4  Jim Jefferies is back and no topic is off limi...   \n",
       "\n",
       "                                          Transcript  \n",
       "0  [slow instrumental music playing] [funk drums ...  \n",
       "1  [siren wailing] I don’t know what you were thi...  \n",
       "2  Join me in welcoming the author of six number ...  \n",
       "3  Premiered on December 13, 2022 Ladies and gent...  \n",
       "4  Please welcome to the stage, Jim Jefferies! He...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/lihuicham/Desktop/Y2S2/BT4222/project/standup-comedy-analysis/main/transcripts.csv')\n",
    "df = df[df.columns[1:]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9dc788f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text) :\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuations, \n",
    "    remove quotation marks, remove words containing numbers, remove \\n'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)   \n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text) \n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "cleaning = lambda x : clean_text(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c63b2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply data cleaning to Transcript column \n",
    "df_clean = df.copy()\n",
    "df_clean['Transcript'] = df_clean['Transcript'].apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7571a480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comedian</th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Subtitle</th>\n",
       "      <th>Transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chris Rock</td>\n",
       "      <td>March 8, 2023</td>\n",
       "      <td>Selective Outrage (2023) | Transcript</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lets go    she said  ill do anything you w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marc Maron</td>\n",
       "      <td>March 3, 2023</td>\n",
       "      <td>Thinky Pain (2013) | Transcript</td>\n",
       "      <td>Marc Maron returns to his old stomping grounds...</td>\n",
       "      <td>i dont know what you were thinking like im no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chelsea Handler</td>\n",
       "      <td>March 3, 2023</td>\n",
       "      <td>Evolution (2020) | Transcript</td>\n",
       "      <td>Chelsea Handler is back and better than ever -...</td>\n",
       "      <td>join me in welcoming the author of six number ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tom Papa</td>\n",
       "      <td>March 3, 2023</td>\n",
       "      <td>What A Day! (2022) | Transcript</td>\n",
       "      <td>Follows Papa as he shares about parenting, his...</td>\n",
       "      <td>premiered on december   ladies and gentlemen g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jim Jefferies</td>\n",
       "      <td>February 22, 2023</td>\n",
       "      <td>High n’ Dry (2023) | Transcript</td>\n",
       "      <td>Jim Jefferies is back and no topic is off limi...</td>\n",
       "      <td>please welcome to the stage jim jefferies hell...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Comedian               Date                                  Title  \\\n",
       "0       Chris Rock      March 8, 2023  Selective Outrage (2023) | Transcript   \n",
       "1       Marc Maron      March 3, 2023        Thinky Pain (2013) | Transcript   \n",
       "2  Chelsea Handler      March 3, 2023          Evolution (2020) | Transcript   \n",
       "3         Tom Papa      March 3, 2023        What A Day! (2022) | Transcript   \n",
       "4    Jim Jefferies  February 22, 2023        High n’ Dry (2023) | Transcript   \n",
       "\n",
       "                                            Subtitle  \\\n",
       "0                                                NaN   \n",
       "1  Marc Maron returns to his old stomping grounds...   \n",
       "2  Chelsea Handler is back and better than ever -...   \n",
       "3  Follows Papa as he shares about parenting, his...   \n",
       "4  Jim Jefferies is back and no topic is off limi...   \n",
       "\n",
       "                                          Transcript  \n",
       "0      lets go    she said  ill do anything you w...  \n",
       "1   i dont know what you were thinking like im no...  \n",
       "2  join me in welcoming the author of six number ...  \n",
       "3  premiered on december   ladies and gentlemen g...  \n",
       "4  please welcome to the stage jim jefferies hell...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35c114c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(415, 5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.shape\n",
    "\n",
    "# there are 415 transcripts in total "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4490f04",
   "metadata": {},
   "source": [
    "## Data Organization \n",
    "\n",
    "We will organised data in two standard text formats : \n",
    "1. **Corpus :** a collection of text where its order is preserved. \n",
    "2. **Document-Term matrix:** implementation of Bag of Words - a collection of words to represent a sentence with word count disregarding the order, in a matrix format. \n",
    "3. **TF-IDF :** reflect how important a word is to a document in a collection or corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3bdf48",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b35fb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle corpus\n",
    "with open('pickle/' + 'corpus.pkl', 'wb') as f:\n",
    "    pickle.dump(df_clean, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6555a11",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3ae96d",
   "metadata": {},
   "source": [
    "## Helper Functions \n",
    "Create own tokenizer for CountVectorizer and TF-IDF Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86051dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag) : \n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # As default pos in lemmatization is Noun\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6941475",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def pos_then_lemmatize(pos_tagged_words) :\n",
    "    res = []\n",
    "    for pos in pos_tagged_words : \n",
    "        word = pos[0]\n",
    "        pos_tag = pos[1]\n",
    "\n",
    "        lem = lemmatizer.lemmatize(word, get_wordnet_pos(pos_tag))\n",
    "        res.append(lem)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0203fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(text) : \n",
    "    words = word_tokenize(text.lower())\n",
    "    \n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    filtered_words = [w for w in words if not w in stop_words] \n",
    "    pos_tagged_words = nltk.pos_tag(filtered_words)\n",
    "    tokens = pos_then_lemmatize(pos_tagged_words)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf479ad",
   "metadata": {},
   "source": [
    "## Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0918d76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aaaaaa</th>\n",
       "      <th>aaaaaaaaaaall</th>\n",
       "      <th>aaaaaaaaah</th>\n",
       "      <th>aaaaaaaahhhhhhh</th>\n",
       "      <th>aaaaaaah</th>\n",
       "      <th>aaaaaaarhhh</th>\n",
       "      <th>aaaaaabout</th>\n",
       "      <th>...</th>\n",
       "      <th>♪with</th>\n",
       "      <th>♪you</th>\n",
       "      <th>♪youse</th>\n",
       "      <th>♪♪</th>\n",
       "      <th>♪♪♪</th>\n",
       "      <th>♫</th>\n",
       "      <th>♫if</th>\n",
       "      <th>♫third</th>\n",
       "      <th>♬</th>\n",
       "      <th>ﬂoor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>415 rows × 52854 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     aa  aaa  aaaa  aaaaaa  aaaaaaaaaaall  aaaaaaaaah  aaaaaaaahhhhhhh  \\\n",
       "0     0    0     0       0              0           0                0   \n",
       "1     0    0     0       0              0           0                0   \n",
       "2     0    0     0       0              0           0                0   \n",
       "3     0    0     0       0              0           0                0   \n",
       "4     0    0     0       0              0           0                0   \n",
       "..   ..  ...   ...     ...            ...         ...              ...   \n",
       "410   0    0     0       0              0           0                0   \n",
       "411   0    0     0       0              0           0                0   \n",
       "412   0    0     0       0              0           0                0   \n",
       "413   0    0     0       0              0           0                0   \n",
       "414   0    0     0       0              0           0                0   \n",
       "\n",
       "     aaaaaaah  aaaaaaarhhh  aaaaaabout  ...  ♪with  ♪you  ♪youse  ♪♪  ♪♪♪  ♫  \\\n",
       "0           0            0           0  ...      0     0       0   0    0  0   \n",
       "1           0            0           0  ...      0     0       0   0    0  0   \n",
       "2           0            0           0  ...      0     0       0   0    0  0   \n",
       "3           0            0           0  ...      0     0       0   0    0  0   \n",
       "4           0            0           0  ...      0     0       0   0    0  0   \n",
       "..        ...          ...         ...  ...    ...   ...     ...  ..  ... ..   \n",
       "410         0            0           0  ...      0     0       0   0    0  0   \n",
       "411         0            0           0  ...      0     0       0   0    0  0   \n",
       "412         0            0           0  ...      0     0       0   0    0  0   \n",
       "413         0            0           0  ...      0     0       0   0    0  0   \n",
       "414         0            0           0  ...      0     0       0   0    0  0   \n",
       "\n",
       "     ♫if  ♫third  ♬  ﬂoor  \n",
       "0      0       0  0     0  \n",
       "1      0       0  0     0  \n",
       "2      0       0  0     0  \n",
       "3      0       0  0     0  \n",
       "4      0       0  0     0  \n",
       "..   ...     ... ..   ...  \n",
       "410    0       0  0     0  \n",
       "411    0       0  0     0  \n",
       "412    0       0  0     0  \n",
       "413    0       0  0     0  \n",
       "414    0       0  0     0  \n",
       "\n",
       "[415 rows x 52854 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count Vectorizer - Document-Term Matrix \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# (1, 2) : include bigram \n",
    "# max_features = 300 : choose features/words that occur most frequently to be its vocabulary \n",
    "cv = CountVectorizer(ngram_range = (1, 1),\n",
    "                    tokenizer = custom_tokenizer)\n",
    "cv_vectors = cv.fit_transform(df_clean['Transcript'])\n",
    "cv_feature_names = cv.get_feature_names_out()\n",
    "cv_matrix = pd.DataFrame(cv_vectors.toarray(), columns=cv_feature_names)\n",
    "cv_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f54e902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle document-term matrix\n",
    "with open('pickle/' + 'dtm.pkl', 'wb') as f:\n",
    "    pickle.dump(cv_matrix, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dbe20e",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54785664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aaaaaa</th>\n",
       "      <th>aaaaaaaaaaall</th>\n",
       "      <th>aaaaaaaaah</th>\n",
       "      <th>aaaaaaaahhhhhhh</th>\n",
       "      <th>aaaaaaah</th>\n",
       "      <th>aaaaaaarhhh</th>\n",
       "      <th>aaaaaabout</th>\n",
       "      <th>...</th>\n",
       "      <th>♪with</th>\n",
       "      <th>♪you</th>\n",
       "      <th>♪youse</th>\n",
       "      <th>♪♪</th>\n",
       "      <th>♪♪♪</th>\n",
       "      <th>♫</th>\n",
       "      <th>♫if</th>\n",
       "      <th>♫third</th>\n",
       "      <th>♬</th>\n",
       "      <th>ﬂoor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>415 rows × 52854 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aa  aaa  aaaa  aaaaaa  aaaaaaaaaaall  aaaaaaaaah  aaaaaaaahhhhhhh  \\\n",
       "0    0.0  0.0   0.0     0.0            0.0         0.0              0.0   \n",
       "1    0.0  0.0   0.0     0.0            0.0         0.0              0.0   \n",
       "2    0.0  0.0   0.0     0.0            0.0         0.0              0.0   \n",
       "3    0.0  0.0   0.0     0.0            0.0         0.0              0.0   \n",
       "4    0.0  0.0   0.0     0.0            0.0         0.0              0.0   \n",
       "..   ...  ...   ...     ...            ...         ...              ...   \n",
       "410  0.0  0.0   0.0     0.0            0.0         0.0              0.0   \n",
       "411  0.0  0.0   0.0     0.0            0.0         0.0              0.0   \n",
       "412  0.0  0.0   0.0     0.0            0.0         0.0              0.0   \n",
       "413  0.0  0.0   0.0     0.0            0.0         0.0              0.0   \n",
       "414  0.0  0.0   0.0     0.0            0.0         0.0              0.0   \n",
       "\n",
       "     aaaaaaah  aaaaaaarhhh  aaaaaabout  ...  ♪with  ♪you  ♪youse   ♪♪  ♪♪♪  \\\n",
       "0         0.0          0.0         0.0  ...    0.0   0.0     0.0  0.0  0.0   \n",
       "1         0.0          0.0         0.0  ...    0.0   0.0     0.0  0.0  0.0   \n",
       "2         0.0          0.0         0.0  ...    0.0   0.0     0.0  0.0  0.0   \n",
       "3         0.0          0.0         0.0  ...    0.0   0.0     0.0  0.0  0.0   \n",
       "4         0.0          0.0         0.0  ...    0.0   0.0     0.0  0.0  0.0   \n",
       "..        ...          ...         ...  ...    ...   ...     ...  ...  ...   \n",
       "410       0.0          0.0         0.0  ...    0.0   0.0     0.0  0.0  0.0   \n",
       "411       0.0          0.0         0.0  ...    0.0   0.0     0.0  0.0  0.0   \n",
       "412       0.0          0.0         0.0  ...    0.0   0.0     0.0  0.0  0.0   \n",
       "413       0.0          0.0         0.0  ...    0.0   0.0     0.0  0.0  0.0   \n",
       "414       0.0          0.0         0.0  ...    0.0   0.0     0.0  0.0  0.0   \n",
       "\n",
       "       ♫  ♫if  ♫third    ♬  ﬂoor  \n",
       "0    0.0  0.0     0.0  0.0   0.0  \n",
       "1    0.0  0.0     0.0  0.0   0.0  \n",
       "2    0.0  0.0     0.0  0.0   0.0  \n",
       "3    0.0  0.0     0.0  0.0   0.0  \n",
       "4    0.0  0.0     0.0  0.0   0.0  \n",
       "..   ...  ...     ...  ...   ...  \n",
       "410  0.0  0.0     0.0  0.0   0.0  \n",
       "411  0.0  0.0     0.0  0.0   0.0  \n",
       "412  0.0  0.0     0.0  0.0   0.0  \n",
       "413  0.0  0.0     0.0  0.0   0.0  \n",
       "414  0.0  0.0     0.0  0.0   0.0  \n",
       "\n",
       "[415 rows x 52854 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer(ngram_range = (1, 1),\n",
    "                    tokenizer = custom_tokenizer)\n",
    "tf_vectors = tf.fit_transform(df_clean['Transcript'])\n",
    "tf_feature_names = tf.get_feature_names_out()\n",
    "tfidf_matrix = pd.DataFrame(tf_vectors.toarray(), columns=tf_feature_names)\n",
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26e95942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle tfidf_matrix\n",
    "with open('pickle/' + 'tfidf.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_matrix, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e772e18",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967986b0",
   "metadata": {},
   "source": [
    "## Testing \n",
    "\n",
    "Use one simple sentece to find out how to process words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95557cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Follows Papa as he shares about parenting his reliance on modern technology rescuing his pet pug and how his marriage has evolved over time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e33f940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['follows', 'papa', 'as', 'he', 'shares', 'about', 'parenting', 'his', 'reliance', 'on', 'modern', 'technology', 'rescuing', 'his', 'pet', 'pug', 'and', 'how', 'his', 'marriage', 'has', 'evolved', 'over', 'time']\n"
     ]
    }
   ],
   "source": [
    "# tokenization \n",
    "\n",
    "words = word_tokenize(sent.lower())\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7342f12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['follows', 'papa', 'shares', 'parenting', 'reliance', 'modern', 'technology', 'rescuing', 'pet', 'pug', 'marriage', 'evolved', 'time']\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords \n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "filtered_words = [w for w in words if not w in stop_words] \n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6b551ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['follows', 'papa', 'share', 'parenting', 'reliance', 'modern', 'technology', 'rescuing', 'pet', 'pug', 'marriage', 'evolved', 'time']\n"
     ]
    }
   ],
   "source": [
    "# lemmitization \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "print(lemmatized_words)\n",
    "\n",
    "# lemmitization by default uses Noun as pos, we need to be more specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d52522b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['follow', 'papa', 'share', 'parent', 'relianc', 'modern', 'technolog', 'rescu', 'pet', 'pug', 'marriag', 'evolv', 'time']\n"
     ]
    }
   ],
   "source": [
    "# stemming\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "print(stemmed_words)\n",
    "\n",
    "# stemming overtruncated the words. use lemmitization instead "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d2fc239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('follows', 'VBZ'), ('papa', 'JJ'), ('shares', 'NNS'), ('parenting', 'VBG'), ('reliance', 'NN'), ('modern', 'JJ'), ('technology', 'NN'), ('rescuing', 'VBG'), ('pet', 'JJ'), ('pug', 'JJ'), ('marriage', 'NN'), ('evolved', 'VBD'), ('time', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# parts of speech tagging \n",
    "pos_tagged_words = nltk.pos_tag(filtered_words)\n",
    "print(pos_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aab1bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a helper function to map the pos tag to wordnet \n",
    "\n",
    "def get_wordnet_pos(treebank_tag) : \n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # As default pos in lemmatization is Noun\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b25da7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['follow', 'papa', 'share', 'parent', 'reliance', 'modern', 'technology', 'rescue', 'pet', 'pug', 'marriage', 'evolve', 'time']\n"
     ]
    }
   ],
   "source": [
    "# pos tagging -> lemmitization \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def pos_then_lemmatize(pos_tagged_words) :\n",
    "    res = []\n",
    "    for pos in pos_tagged_words : \n",
    "        word = pos[0]\n",
    "        pos_tag = pos[1]\n",
    "\n",
    "        lem = lemmatizer.lemmatize(word, get_wordnet_pos(pos_tag))\n",
    "        res.append(lem)\n",
    "    return res\n",
    "\n",
    "print(pos_then_lemmatize(pos_tagged_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cf948b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join back the processed words \n",
    "processed_words = pos_then_lemmatize(pos_tagged_words)\n",
    "new_sent = ' '.join(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48c63f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evolve</th>\n",
       "      <th>follow</th>\n",
       "      <th>marriage</th>\n",
       "      <th>modern</th>\n",
       "      <th>papa</th>\n",
       "      <th>parent</th>\n",
       "      <th>pet</th>\n",
       "      <th>pug</th>\n",
       "      <th>reliance</th>\n",
       "      <th>rescue</th>\n",
       "      <th>share</th>\n",
       "      <th>technology</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   evolve  follow  marriage  modern  papa  parent  pet  pug  reliance  rescue  \\\n",
       "0       1       1         1       1     1       1    1    1         1       1   \n",
       "\n",
       "   share  technology  time  \n",
       "0      1           1     1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count Vectorizer - Document-Term Matrix \n",
    "\n",
    "cv_test = CountVectorizer(ngram_range = (1, 1), stop_words='english')  # (1, 2) is bigram \n",
    "cv_vectors_test = cv_test.fit_transform([new_sent])\n",
    "cv_feature_names_test = cv_test.get_feature_names_out()\n",
    "cv_matrix_test = pd.DataFrame(cv_vectors_test.toarray(), columns=cv_feature_names_test)\n",
    "cv_matrix_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37b2fc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evolve</th>\n",
       "      <th>follow</th>\n",
       "      <th>marriage</th>\n",
       "      <th>modern</th>\n",
       "      <th>papa</th>\n",
       "      <th>parent</th>\n",
       "      <th>pet</th>\n",
       "      <th>pug</th>\n",
       "      <th>reliance</th>\n",
       "      <th>rescue</th>\n",
       "      <th>share</th>\n",
       "      <th>technology</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.27735</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>0.27735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    evolve   follow  marriage   modern     papa   parent      pet      pug  \\\n",
       "0  0.27735  0.27735   0.27735  0.27735  0.27735  0.27735  0.27735  0.27735   \n",
       "\n",
       "   reliance   rescue    share  technology     time  \n",
       "0   0.27735  0.27735  0.27735     0.27735  0.27735  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF Vectorizer\n",
    "\n",
    "tf_test = TfidfVectorizer(ngram_range = (1, 1))\n",
    "tf_vectors_test = tf_test.fit_transform([new_sent])\n",
    "tf_feature_names_test = tf_test.get_feature_names_out()\n",
    "tfidf_matrix_test = pd.DataFrame(tf_vectors_test.toarray(), columns=tf_feature_names_test)\n",
    "tfidf_matrix_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
