{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "330ace7e",
   "metadata": {},
   "source": [
    "# More Data Cleaning \n",
    "\n",
    "We realised that the document-term matrices we created in 2-Data-Cleaning.ipynb using Count Vectorizer and TF-IDF Vectorizer has a lot of meaningless filler words and common words such as `'like'`, `'just'`, `'people'`, `'youre'` and etc. \n",
    "\n",
    "Therefore, we wish to inspect the matrices further and create a new stop words list in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66002881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import re\n",
    "import string \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f50f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the document-term matrix formed by Count Vectorizer \n",
    "df_cv = pd.read_pickle('/Users/lihuicham/Desktop/Y2S2/BT4222/project/standup-comedy-analysis/main/pickle/cv.pkl')\n",
    "# transpose to term-document matrix \n",
    "df_cv = df_cv.transpose()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9241b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 50 words in each transcript \n",
    "top_dict = {}\n",
    "for c in df_cv.columns:\n",
    "    top = df_cv[c].sort_values(ascending=False).head(50)\n",
    "    top_dict[c]= list(zip(top.index, top.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21c7a0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 50 words in each transcript \n",
    "# for transcript, top_words in top_dict.items():\n",
    "#     print(transcript)\n",
    "#     print(', '.join([word for word, count in top_words[0:49]]))\n",
    "#     print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac40ff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add the most common top words to a stop word list\n",
    "\n",
    "# Let's first pull out the top 50 words for each comedian\n",
    "words = []\n",
    "for transcript in df_cv.columns:\n",
    "    top = [word for (word, count) in top_dict[transcript]]\n",
    "    for t in top:\n",
    "        words.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0747191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('get', 404),\n",
       " ('go', 404),\n",
       " ('know', 401),\n",
       " ('dont', 400),\n",
       " ('im', 398),\n",
       " ('like', 396),\n",
       " ('say', 391),\n",
       " ('thats', 389),\n",
       " ('one', 383),\n",
       " ('come', 363),\n",
       " ('right', 357),\n",
       " ('think', 352),\n",
       " ('youre', 346),\n",
       " ('people', 338),\n",
       " ('see', 332),\n",
       " ('look', 327),\n",
       " ('want', 321),\n",
       " ('time', 315),\n",
       " ('make', 307),\n",
       " ('na', 295),\n",
       " ('gon', 272),\n",
       " ('thing', 270),\n",
       " ('oh', 263),\n",
       " ('take', 251),\n",
       " ('good', 249),\n",
       " ('guy', 249),\n",
       " ('fuck', 243),\n",
       " ('would', 227),\n",
       " ('yeah', 227),\n",
       " ('tell', 227),\n",
       " ('well', 225),\n",
       " ('he', 197),\n",
       " ('shit', 196),\n",
       " ('cause', 195),\n",
       " ('back', 194),\n",
       " ('theyre', 191),\n",
       " ('man', 188),\n",
       " ('really', 173),\n",
       " ('cant', 170),\n",
       " ('little', 167),\n",
       " ('let', 150),\n",
       " ('love', 145),\n",
       " ('okay', 136),\n",
       " ('give', 133),\n",
       " ('never', 130),\n",
       " ('day', 129),\n",
       " ('even', 127),\n",
       " ('didnt', 125),\n",
       " ('kid', 120),\n",
       " ('mean', 120),\n",
       " ('woman', 117),\n",
       " ('year', 114),\n",
       " ('show', 110),\n",
       " ('way', 105),\n",
       " ('ive', 105),\n",
       " ('♪', 102),\n",
       " ('put', 100),\n",
       " ('talk', 99),\n",
       " ('call', 88),\n",
       " ('shes', 84),\n",
       " ('ill', 83),\n",
       " ('hey', 83),\n",
       " ('–', 80),\n",
       " ('try', 79),\n",
       " ('ever', 79),\n",
       " ('thank', 79),\n",
       " ('there', 78),\n",
       " ('feel', 76),\n",
       " ('need', 75),\n",
       " ('white', 73),\n",
       " ('could', 73),\n",
       " ('something', 73),\n",
       " ('god', 73),\n",
       " ('walk', 71),\n",
       " ('lot', 70),\n",
       " ('start', 66),\n",
       " ('girl', 65),\n",
       " ('life', 61),\n",
       " ('every', 61),\n",
       " ('black', 60),\n",
       " ('two', 60),\n",
       " ('happen', 59),\n",
       " ('uh', 59),\n",
       " ('u', 56),\n",
       " ('ta', 54),\n",
       " ('work', 52),\n",
       " ('baby', 50),\n",
       " ('always', 50),\n",
       " ('much', 49),\n",
       " ('joke', 48),\n",
       " ('big', 48),\n",
       " ('first', 47),\n",
       " ('old', 47),\n",
       " ('around', 44),\n",
       " ('aint', 42),\n",
       " ('great', 42),\n",
       " ('friend', 41),\n",
       " ('wan', 40),\n",
       " ('dad', 40),\n",
       " ('bad', 39),\n",
       " ('dog', 37),\n",
       " ('wife', 35),\n",
       " ('kind', 35),\n",
       " ('mom', 35),\n",
       " ('g', 35),\n",
       " ('motherfucker', 34),\n",
       " ('dick', 33),\n",
       " ('watch', 33),\n",
       " ('dude', 32),\n",
       " ('use', 32),\n",
       " ('house', 28),\n",
       " ('bit', 28),\n",
       " ('night', 27),\n",
       " ('yes', 26),\n",
       " ('whats', 26),\n",
       " ('n', 26),\n",
       " ('—', 25),\n",
       " ('um', 25),\n",
       " ('new', 25),\n",
       " ('gay', 24),\n",
       " ('ha', 24),\n",
       " ('car', 23),\n",
       " ('live', 23),\n",
       " ('bitch', 22),\n",
       " ('yall', 22),\n",
       " ('laugh', 22),\n",
       " ('name', 21),\n",
       " ('eat', 21),\n",
       " ('home', 20),\n",
       " ('as', 20),\n",
       " ('doesnt', 20),\n",
       " ('sit', 19),\n",
       " ('men', 19),\n",
       " ('kill', 18),\n",
       " ('die', 18),\n",
       " ('sex', 18),\n",
       " ('ok', 18),\n",
       " ('fuckin', 17),\n",
       " ('id', 17),\n",
       " ('money', 17),\n",
       " ('ask', 17),\n",
       " ('everybody', 16),\n",
       " ('youve', 16),\n",
       " ('find', 16),\n",
       " ('run', 16),\n",
       " ('play', 16),\n",
       " ('lady', 16),\n",
       " ('country', 16),\n",
       " ('remember', 15),\n",
       " ('boy', 15),\n",
       " ('room', 15),\n",
       " ('movie', 15),\n",
       " ('president', 15),\n",
       " ('doctor', 15),\n",
       " ('e', 15),\n",
       " ('pussy', 14),\n",
       " ('audience', 14),\n",
       " ('son', 14),\n",
       " ('stuff', 14),\n",
       " ('world', 14),\n",
       " ('weird', 13),\n",
       " ('keep', 13),\n",
       " ('real', 13),\n",
       " ('someone', 13),\n",
       " ('face', 13),\n",
       " ('hear', 13),\n",
       " ('hand', 13),\n",
       " ('word', 13),\n",
       " ('crazy', 13),\n",
       " ('school', 12),\n",
       " ('leave', 12),\n",
       " ('drink', 12),\n",
       " ('fcking', 12),\n",
       " ('trump', 12),\n",
       " ('jesus', 12),\n",
       " ('la', 12),\n",
       " ('hell', 12),\n",
       " ('believe', 12),\n",
       " ('child', 11),\n",
       " ('still', 11),\n",
       " ('gun', 11),\n",
       " ('actually', 11),\n",
       " ('tonight', 11),\n",
       " ('fck', 11),\n",
       " ('maybe', 11),\n",
       " ('nice', 11),\n",
       " ('em', 11),\n",
       " ('sleep', 10),\n",
       " ('goddamn', 10),\n",
       " ('parent', 10),\n",
       " ('america', 10),\n",
       " ('drug', 10),\n",
       " ('phone', 10),\n",
       " ('american', 10),\n",
       " ('write', 10),\n",
       " ('stop', 10),\n",
       " ('party', 10),\n",
       " ('da', 10),\n",
       " ('mi', 10),\n",
       " ('le', 10),\n",
       " ('family', 9),\n",
       " ('story', 9),\n",
       " ('sorry', 9),\n",
       " ('ball', 9),\n",
       " ('song', 9),\n",
       " ('also', 9),\n",
       " ('yo', 9),\n",
       " ('hate', 9),\n",
       " ('somebody', 9),\n",
       " ('fucking', 9),\n",
       " ('un', 9),\n",
       " ('se', 9),\n",
       " ('con', 9),\n",
       " ('una', 9),\n",
       " ('si', 9),\n",
       " ('ngga', 8),\n",
       " ('date', 8),\n",
       " ('cat', 8),\n",
       " ('nothing', 8),\n",
       " ('please', 8),\n",
       " ('fine', 8),\n",
       " ('cheer', 8),\n",
       " ('person', 8),\n",
       " ('whole', 8),\n",
       " ('fat', 8),\n",
       " ('mother', 8),\n",
       " ('last', 8),\n",
       " ('york', 8),\n",
       " ('cunt', 8),\n",
       " ('three', 8),\n",
       " ('co', 8),\n",
       " ('wait', 8),\n",
       " ('lo', 8),\n",
       " ('al', 8),\n",
       " ('del', 8),\n",
       " ('che', 8),\n",
       " ('eye', 7),\n",
       " ('bed', 7),\n",
       " ('alright', 7),\n",
       " ('sound', 7),\n",
       " ('everyone', 7),\n",
       " ('laughter', 7),\n",
       " ('stand', 7),\n",
       " ('minute', 7),\n",
       " ('r', 7),\n",
       " ('cool', 7),\n",
       " ('war', 7),\n",
       " ('fight', 7),\n",
       " ('di', 7),\n",
       " ('non', 7),\n",
       " ('sono', 7),\n",
       " ('il', 7),\n",
       " ('per', 7),\n",
       " ('cosa', 7),\n",
       " ('cazzo', 7),\n",
       " ('questo', 7),\n",
       " ('indian', 6),\n",
       " ('comedy', 6),\n",
       " ('hed', 6),\n",
       " ('brother', 6),\n",
       " ('cop', 6),\n",
       " ('food', 6),\n",
       " ('news', 6),\n",
       " ('asian', 6),\n",
       " ('turn', 6),\n",
       " ('drive', 6),\n",
       " ('er', 6),\n",
       " ('city', 6),\n",
       " ('huh', 6),\n",
       " ('course', 6),\n",
       " ('bar', 6),\n",
       " ('horse', 6),\n",
       " ('daddy', 6),\n",
       " ('job', 6),\n",
       " ('girlfriend', 6),\n",
       " ('sort', 6),\n",
       " ('dead', 6),\n",
       " ('saw', 6),\n",
       " ('nobody', 6),\n",
       " ('problem', 6),\n",
       " ('ho', 6),\n",
       " ('fart', 6),\n",
       " ('bill', 6),\n",
       " ('ci', 6),\n",
       " ('gli', 6),\n",
       " ('water', 5),\n",
       " ('anything', 5),\n",
       " ('nggas', 5),\n",
       " ('understand', 5),\n",
       " ('porn', 5),\n",
       " ('cream', 5),\n",
       " ('mr', 5),\n",
       " ('husband', 5),\n",
       " ('everything', 5),\n",
       " ('funny', 5),\n",
       " ('bullshit', 5),\n",
       " ('head', 5),\n",
       " ('do', 5),\n",
       " ('wear', 5),\n",
       " ('listen', 5),\n",
       " ('hello', 5),\n",
       " ('suck', 5),\n",
       " ('mate', 5),\n",
       " ('care', 5),\n",
       " ('hollywood', 5),\n",
       " ('buy', 5),\n",
       " ('question', 5),\n",
       " ('away', 5),\n",
       " ('ah', 5),\n",
       " ('quite', 5),\n",
       " ('egg', 5),\n",
       " ('another', 5),\n",
       " ('place', 5),\n",
       " ('solo', 5),\n",
       " ('è', 5),\n",
       " ('perché', 5),\n",
       " ('della', 5),\n",
       " ('sapete', 5),\n",
       " ('tit', 5),\n",
       " ('mai', 5),\n",
       " ('point', 4),\n",
       " ('jew', 4),\n",
       " ('marry', 4),\n",
       " ('hour', 4),\n",
       " ('dance', 4),\n",
       " ('young', 4),\n",
       " ('speak', 4),\n",
       " ('btch', 4),\n",
       " ('mama', 4),\n",
       " ('grandma', 4),\n",
       " ('jim', 4),\n",
       " ('chinese', 4),\n",
       " ('body', 4),\n",
       " ('police', 4),\n",
       " ('george', 4),\n",
       " ('chris', 4),\n",
       " ('hard', 4),\n",
       " ('end', 4),\n",
       " ('fun', 4),\n",
       " ('anyway', 4),\n",
       " ('check', 4),\n",
       " ('weve', 4),\n",
       " ('state', 4),\n",
       " ('month', 4),\n",
       " ('true', 4),\n",
       " ('sun', 4),\n",
       " ('john', 4),\n",
       " ('jack', 4),\n",
       " ('wash', 4),\n",
       " ('father', 4),\n",
       " ('bear', 4),\n",
       " ('learn', 4),\n",
       " ('game', 4),\n",
       " ('award', 4),\n",
       " ('kevin', 4),\n",
       " ('pretty', 4),\n",
       " ('abortion', 4),\n",
       " ('might', 4),\n",
       " ('reason', 4),\n",
       " ('comedian', 4),\n",
       " ('damn', 4),\n",
       " ('motherfucking', 4),\n",
       " ('joe', 4),\n",
       " ('mum', 4),\n",
       " ('anthony', 4),\n",
       " ('lie', 4),\n",
       " ('next', 4),\n",
       " ('donald', 4),\n",
       " ('io', 4),\n",
       " ('ti', 4),\n",
       " ('tutto', 4),\n",
       " ('sei', 4),\n",
       " ('più', 4),\n",
       " ('questa', 4),\n",
       " ('tutti', 4),\n",
       " ('ya', 4),\n",
       " ('quando', 4),\n",
       " ('hair', 3),\n",
       " ('tax', 3),\n",
       " ('mad', 3),\n",
       " ('picture', 3),\n",
       " ('ice', 3),\n",
       " ('relationship', 3),\n",
       " ('trevor', 3),\n",
       " ('applaud', 3),\n",
       " ('sht', 3),\n",
       " ('secret', 3),\n",
       " ('shut', 3),\n",
       " ('stupid', 3),\n",
       " ('book', 3),\n",
       " ('language', 3),\n",
       " ('human', 3),\n",
       " ('morning', 3),\n",
       " ('applause', 3),\n",
       " ('fckin', 3),\n",
       " ('smoke', 3),\n",
       " ('town', 3),\n",
       " ('vote', 3),\n",
       " ('plan', 3),\n",
       " ('stick', 3),\n",
       " ('door', 3),\n",
       " ('hotel', 3),\n",
       " ('tom', 3),\n",
       " ('milk', 3),\n",
       " ('front', 3),\n",
       " ('cheese', 3),\n",
       " ('prolife', 3),\n",
       " ('folk', 3),\n",
       " ('beautiful', 3),\n",
       " ('sir', 3),\n",
       " ('russell', 3),\n",
       " ('india', 3),\n",
       " ('street', 3),\n",
       " ('sing', 3),\n",
       " ('isnt', 3),\n",
       " ('pregnant', 3),\n",
       " ('sometimes', 3),\n",
       " ('win', 3),\n",
       " ('tv', 3),\n",
       " ('club', 3),\n",
       " ('snake', 3),\n",
       " ('pizza', 3),\n",
       " ('moment', 3),\n",
       " ('wed', 3),\n",
       " ('asshole', 3),\n",
       " ('obama', 3),\n",
       " ('dave', 3),\n",
       " ('ima', 3),\n",
       " ('chicken', 3),\n",
       " ('wouldnt', 3),\n",
       " ('racist', 3),\n",
       " ('golden', 3),\n",
       " ('globe', 3),\n",
       " ('film', 3),\n",
       " ('pay', 3),\n",
       " ('host', 3),\n",
       " ('fly', 3),\n",
       " ('meet', 3),\n",
       " ('king', 3),\n",
       " ('hitler', 3),\n",
       " ('today', 3),\n",
       " ('carlin', 3),\n",
       " ('stage', 3),\n",
       " ('help', 3),\n",
       " ('bag', 3),\n",
       " ('brain', 3),\n",
       " ('de', 3),\n",
       " ('michael', 3),\n",
       " ('el', 3),\n",
       " ('era', 3),\n",
       " ('su', 3),\n",
       " ('te', 3),\n",
       " ('casa', 3),\n",
       " ('fatto', 3),\n",
       " ('mio', 3),\n",
       " ('fare', 3),\n",
       " ('l', 3),\n",
       " ('paul', 3),\n",
       " ('sport', 3),\n",
       " ('hanno', 3),\n",
       " ('bambino', 3),\n",
       " ('essere', 3),\n",
       " ('loro', 3),\n",
       " ('dan', 2),\n",
       " ('change', 2),\n",
       " ('pop', 2),\n",
       " ('jewish', 2),\n",
       " ('art', 2),\n",
       " ('theater', 2),\n",
       " ('martin', 2),\n",
       " ('wish', 2),\n",
       " ('many', 2),\n",
       " ('ohh', 2),\n",
       " ('dumb', 2),\n",
       " ('cooper', 2),\n",
       " ('motherfcker', 2),\n",
       " ('btches', 2),\n",
       " ('ooh', 2),\n",
       " ('mike', 2),\n",
       " ('pool', 2),\n",
       " ('biden', 2),\n",
       " ('journalist', 2),\n",
       " ('cheat', 2),\n",
       " ('ali', 2),\n",
       " ('banana', 2),\n",
       " ('chuckle', 2),\n",
       " ('bro', 2),\n",
       " ('prayer', 2),\n",
       " ('kathleen', 2),\n",
       " ('accent', 2),\n",
       " ('murder', 2),\n",
       " ('santa', 2),\n",
       " ('seat', 2),\n",
       " ('africa', 2),\n",
       " ('pink', 2),\n",
       " ('different', 2),\n",
       " ('cure', 2),\n",
       " ('football', 2),\n",
       " ('suicide', 2),\n",
       " ('fire', 2),\n",
       " ('sarah', 2),\n",
       " ('♫', 2),\n",
       " ('bo', 2),\n",
       " ('artist', 2),\n",
       " ('holy', 2),\n",
       " ('kenny', 2),\n",
       " ('offend', 2),\n",
       " ('mask', 2),\n",
       " ('anybody', 2),\n",
       " ('rule', 2),\n",
       " ('dream', 2),\n",
       " ('license', 2),\n",
       " ('photo', 2),\n",
       " ('train', 2),\n",
       " ('honey', 2),\n",
       " ('doin', 2),\n",
       " ('loser', 2),\n",
       " ('lose', 2),\n",
       " ('mommy', 2),\n",
       " ('late', 2),\n",
       " ('cross', 2),\n",
       " ('become', 2),\n",
       " ('five', 2),\n",
       " ('captain', 2),\n",
       " ('cuss', 2),\n",
       " ('weed', 2),\n",
       " ('star', 2),\n",
       " ('i–', 2),\n",
       " ('bottle', 2),\n",
       " ('box', 2),\n",
       " ('hasan', 2),\n",
       " ('nose', 2),\n",
       " ('key', 2),\n",
       " ('filipino', 2),\n",
       " ('lee', 2),\n",
       " ('couple', 2),\n",
       " ('planet', 2),\n",
       " ('massage', 2),\n",
       " ('attack', 2),\n",
       " ('limp', 2),\n",
       " ('must', 2),\n",
       " ('ox', 2),\n",
       " ('massacre', 2),\n",
       " ('arm', 2),\n",
       " ('carlins', 2),\n",
       " ('ila', 2),\n",
       " ('mouth', 2),\n",
       " ('ryan', 2),\n",
       " ('high', 2),\n",
       " ('special', 2),\n",
       " ('raise', 2),\n",
       " ('dc', 2),\n",
       " ('long', 2),\n",
       " ('enough', 2),\n",
       " ('wasnt', 2),\n",
       " ('shite', 2),\n",
       " ('wee', 2),\n",
       " ('rice', 2),\n",
       " ('glasgow', 2),\n",
       " ('period', 2),\n",
       " ('cuz', 2),\n",
       " ('couch', 2),\n",
       " ('amaze', 2),\n",
       " ('restaurant', 2),\n",
       " ('english', 2),\n",
       " ('blow', 2),\n",
       " ('fall', 2),\n",
       " ('far', 2),\n",
       " ('move', 2),\n",
       " ('youll', 2),\n",
       " ('mall', 2),\n",
       " ('land', 2),\n",
       " ('protect', 2),\n",
       " ('beef', 2),\n",
       " ('wipe', 2),\n",
       " ('act', 2),\n",
       " ('bathroom', 2),\n",
       " ('robot', 2),\n",
       " ('aziz', 2),\n",
       " ('eddie', 2),\n",
       " ('panda', 2),\n",
       " ('coffee', 2),\n",
       " ('wolf', 2),\n",
       " ('nominate', 2),\n",
       " ('welcome', 2),\n",
       " ('press', 2),\n",
       " ('foreign', 2),\n",
       " ('sure', 2),\n",
       " ('answer', 2),\n",
       " ('actor', 2),\n",
       " ('german', 2),\n",
       " ('frankie', 2),\n",
       " ('side', 2),\n",
       " ('babe', 2),\n",
       " ('history', 2),\n",
       " ('steve', 2),\n",
       " ('wont', 2),\n",
       " ('religion', 2),\n",
       " ('wallet', 2),\n",
       " ('cute', 2),\n",
       " ('isi', 2),\n",
       " ('band', 2),\n",
       " ('reach', 2),\n",
       " ('hurt', 2),\n",
       " ('hope', 2),\n",
       " ('album', 2),\n",
       " ('racism', 2),\n",
       " ('best', 2),\n",
       " ('blue', 2),\n",
       " ('kiss', 2),\n",
       " ('jail', 2),\n",
       " ('birmingham', 2),\n",
       " ('number', 2),\n",
       " ('david', 2),\n",
       " ('death', 2),\n",
       " ('umm', 2),\n",
       " ('hit', 2),\n",
       " ('pope', 2),\n",
       " ('pull', 2),\n",
       " ('goin', 2),\n",
       " ('throw', 2),\n",
       " ('wedding', 2),\n",
       " ('queen', 2),\n",
       " ('save', 2),\n",
       " ('harry', 2),\n",
       " ('magician', 2),\n",
       " ('immigrant', 2),\n",
       " ('argument', 2),\n",
       " ('bread', 2),\n",
       " ('bounce', 2),\n",
       " ('medical', 2),\n",
       " ('roll', 2),\n",
       " ('nut', 2),\n",
       " ('sick', 2),\n",
       " ('part', 2),\n",
       " ('wow', 2),\n",
       " ('mimic', 2),\n",
       " ('voice', 2),\n",
       " ('air', 2),\n",
       " ('british', 2),\n",
       " ('que', 2),\n",
       " ('en', 2),\n",
       " ('los', 2),\n",
       " ('pero', 2),\n",
       " ('por', 2),\n",
       " ('para', 2),\n",
       " ('como', 2),\n",
       " ('todos', 2),\n",
       " ('así', 2),\n",
       " ('qué', 2),\n",
       " ('mierda', 2),\n",
       " ('muy', 2),\n",
       " ('cuando', 2),\n",
       " ('sé', 2),\n",
       " ('porque', 2),\n",
       " ('bien', 2),\n",
       " ('¿qué', 2),\n",
       " ('iceberg', 2),\n",
       " ('poi', 2),\n",
       " ('così', 2),\n",
       " ('po', 2),\n",
       " ('hai', 2),\n",
       " ('grow', 2),\n",
       " ('woo', 2),\n",
       " ('shark', 2),\n",
       " ('cos', 2),\n",
       " ('mouse', 2),\n",
       " ('buck', 2),\n",
       " ('republican', 2),\n",
       " ('richard', 2),\n",
       " ('sexual', 2),\n",
       " ('clinton', 2),\n",
       " ('blind', 2),\n",
       " ('os', 2),\n",
       " ('australia', 2),\n",
       " ('sempre', 2),\n",
       " ('ogni', 2),\n",
       " ('tu', 2),\n",
       " ('quel', 2),\n",
       " ('dei', 2),\n",
       " ('bible', 2),\n",
       " ('nel', 2),\n",
       " ('gorilla', 2),\n",
       " ('plane', 2),\n",
       " ('repeat', 2),\n",
       " ('free', 2),\n",
       " ('cock', 2),\n",
       " ('list', 2),\n",
       " ('cocksucker', 2),\n",
       " ('fish', 2),\n",
       " ('thought', 2),\n",
       " ('piss', 2),\n",
       " ('ahah', 2),\n",
       " ('piu', 2),\n",
       " ('gente', 2),\n",
       " ('grazie', 2),\n",
       " ('ce', 2),\n",
       " ('perche', 2),\n",
       " ('bene', 2),\n",
       " ('vi', 2),\n",
       " ('dio', 2),\n",
       " ('lola', 1),\n",
       " ('shoe', 1),\n",
       " ('bin', 1),\n",
       " ('kanye', 1),\n",
       " ('honest', 1),\n",
       " ('chain', 1),\n",
       " ('wake', 1),\n",
       " ('midterm', 1),\n",
       " ('ellington', 1),\n",
       " ('student', 1),\n",
       " ('and–', 1),\n",
       " ('bra', 1),\n",
       " ('hook', 1),\n",
       " ('jax', 1),\n",
       " ('liberal', 1),\n",
       " ('curry', 1),\n",
       " ('order', 1),\n",
       " ('normal', 1),\n",
       " ('pubes', 1),\n",
       " ('threat', 1),\n",
       " ('applauds', 1),\n",
       " ('twain', 1),\n",
       " ('fragility', 1),\n",
       " ('theres', 1),\n",
       " ('ayy', 1),\n",
       " ('tittyfuck', 1),\n",
       " ('buttcheeks', 1),\n",
       " ('norm', 1),\n",
       " ('anyways', 1),\n",
       " ('motherfcking', 1),\n",
       " ('hoo', 1),\n",
       " ('event', 1),\n",
       " ('upstate', 1),\n",
       " ('indianapolis', 1),\n",
       " ('poor', 1),\n",
       " ('hi', 1),\n",
       " ('camp', 1),\n",
       " ('mandy', 1),\n",
       " ('cnn', 1),\n",
       " ('medium', 1),\n",
       " ('cum', 1),\n",
       " ('fan', 1),\n",
       " ('week', 1),\n",
       " ('pandemic', 1),\n",
       " ('march', 1),\n",
       " ('shower', 1),\n",
       " ('hummus', 1),\n",
       " ('community', 1),\n",
       " ('trans', 1),\n",
       " ('missouri', 1),\n",
       " ('nah', 1),\n",
       " ('wang', 1),\n",
       " ('cork', 1),\n",
       " ('officer', 1),\n",
       " ('dorner', 1),\n",
       " ('chappelle', 1),\n",
       " ('eight', 1),\n",
       " ('instagram', 1),\n",
       " ('claus', 1),\n",
       " ('emergency', 1),\n",
       " ('crime', 1),\n",
       " ('ticket', 1),\n",
       " ('mushroom', 1),\n",
       " ('color', 1),\n",
       " ('victim', 1),\n",
       " ('midget', 1),\n",
       " ('brown', 1),\n",
       " ('citizen', 1),\n",
       " ('cripple', 1),\n",
       " ('challenge', 1),\n",
       " ('becomes', 1),\n",
       " ('dark', 1),\n",
       " ('vertically', 1),\n",
       " ('africanamerican', 1),\n",
       " ('differentlyabled', 1),\n",
       " ('cancer', 1),\n",
       " ('interest', 1),\n",
       " ('scooter', 1),\n",
       " ('everythings', 1),\n",
       " ('single', 1),\n",
       " ('piece', 1),\n",
       " ('light', 1),\n",
       " ('erm', 1),\n",
       " ('lockdown', 1),\n",
       " ('daggum', 1),\n",
       " ('walmart', 1),\n",
       " ('mick', 1),\n",
       " ('fadoodle', 1),\n",
       " ('doodle', 1),\n",
       " ('jagger', 1),\n",
       " ('bop', 1),\n",
       " ('laughing', 1),\n",
       " ('helicopter', 1),\n",
       " ('coronavirus', 1),\n",
       " ('government', 1),\n",
       " ('tuesday', 1),\n",
       " ('serious', 1),\n",
       " ('covid', 1),\n",
       " ('june', 1),\n",
       " ('pride', 1),\n",
       " ('cancel', 1),\n",
       " ('grandparent', 1),\n",
       " ('elderly', 1),\n",
       " ('grandmother', 1),\n",
       " ('nana', 1),\n",
       " ('continue', 1),\n",
       " ('dmv', 1),\n",
       " ('downtown', 1),\n",
       " ('mulaney', 1),\n",
       " ('flight', 1),\n",
       " ('pm', 1),\n",
       " ('sean', 1),\n",
       " ('connery', 1),\n",
       " ('youth', 1),\n",
       " ('sign', 1),\n",
       " ('wine', 1),\n",
       " ('latino', 1),\n",
       " ('letterman', 1),\n",
       " ('hick', 1),\n",
       " ('nonsmoker', 1),\n",
       " ('heather', 1),\n",
       " ('remove', 1),\n",
       " ('belief', 1),\n",
       " ('finally', 1),\n",
       " ('absolutely', 1),\n",
       " ('later', 1),\n",
       " ('appear', 1),\n",
       " ('ought', 1),\n",
       " ('scrub', 1),\n",
       " ('nugget', 1),\n",
       " ('deal', 1),\n",
       " ('scene', 1),\n",
       " ('tough', 1),\n",
       " ('ralphie', 1),\n",
       " ('may', 1),\n",
       " ('coach', 1),\n",
       " ('doug', 1),\n",
       " ('close', 1),\n",
       " ('stanhope', 1),\n",
       " ('barb', 1),\n",
       " ('maid', 1),\n",
       " ('ring', 1),\n",
       " ('connolly', 1),\n",
       " ('bandage', 1),\n",
       " ('douglas', 1),\n",
       " ('autism', 1),\n",
       " ('honestly', 1),\n",
       " ('bike', 1),\n",
       " ('dennys', 1),\n",
       " ('hike', 1),\n",
       " ('wallpaper', 1),\n",
       " ('peter', 1),\n",
       " ('twin', 1),\n",
       " ('jimmy', 1),\n",
       " ('josep', 1),\n",
       " ('jo', 1),\n",
       " ('ka', 1),\n",
       " ('half', 1),\n",
       " ('bloke', 1),\n",
       " ('denver', 1),\n",
       " ('tj', 1),\n",
       " ('jerry', 1),\n",
       " ('text', 1),\n",
       " ('pitbull', 1),\n",
       " ('difficult', 1),\n",
       " ('oprah', 1),\n",
       " ('rescue', 1),\n",
       " ('pound', 1),\n",
       " ('trumpedup', 1),\n",
       " ('retard', 1),\n",
       " ('blog', 1),\n",
       " ('hop', 1),\n",
       " ('turk', 1),\n",
       " ('grateful', 1),\n",
       " ('scott', 1),\n",
       " ('hole', 1),\n",
       " ('ghost', 1),\n",
       " ('mary', 1),\n",
       " ('bless', 1),\n",
       " ('cupboard', 1),\n",
       " ('terrify', 1),\n",
       " ('four', 1),\n",
       " ('everlasting', 1),\n",
       " ('sister', 1),\n",
       " ('imagine', 1),\n",
       " ('fellow', 1),\n",
       " ('wind', 1),\n",
       " ('gangster', 1),\n",
       " ('pant', 1),\n",
       " ('pass', 1),\n",
       " ('sergeant', 1),\n",
       " ('cloth', 1),\n",
       " ('loirn', 1),\n",
       " ('feather', 1),\n",
       " ('knock', 1),\n",
       " ('indians', 1),\n",
       " ('youse', 1),\n",
       " ('fighter', 1),\n",
       " ('therell', 1),\n",
       " ('however', 1),\n",
       " ('western', 1),\n",
       " ('horseplay', 1),\n",
       " ('test', 1),\n",
       " ('georgia', 1),\n",
       " ('marshmallow', 1),\n",
       " ('trainer', 1),\n",
       " ('just–', 1),\n",
       " ('grizzly', 1),\n",
       " ('tim', 1),\n",
       " ('behind', 1),\n",
       " ('trouble', 1),\n",
       " ('ass', 1),\n",
       " ('engage', 1),\n",
       " ('shrooms', 1),\n",
       " ('delete', 1),\n",
       " ('harriet', 1),\n",
       " ('window', 1),\n",
       " ('tubman', 1),\n",
       " ('vagenda', 1),\n",
       " ('anthem', 1),\n",
       " ('canyon', 1),\n",
       " ('thatcher', 1),\n",
       " ('carpet', 1),\n",
       " ('jungle', 1),\n",
       " ('remnant', 1),\n",
       " ('£', 1),\n",
       " ('southend', 1),\n",
       " ('uber', 1),\n",
       " ('present', 1),\n",
       " ('imitates', 1),\n",
       " ('marathon', 1),\n",
       " ('billy', 1),\n",
       " ('griot', 1),\n",
       " ('genre', 1),\n",
       " ('form', 1),\n",
       " ('netflix', 1),\n",
       " ('sequel', 1),\n",
       " ('anymore', 1),\n",
       " ('johnny', 1),\n",
       " ('kev', 1),\n",
       " ('shop', 1),\n",
       " ('bird', 1),\n",
       " ('penis', 1),\n",
       " ('tiffany', 1),\n",
       " ('jen', 1),\n",
       " ('oona', 1),\n",
       " ('chef', 1),\n",
       " ('bride', 1),\n",
       " ('toe', 1),\n",
       " ('lotion', 1),\n",
       " ('foreigner', 1),\n",
       " ('table', 1),\n",
       " ('wifes', 1),\n",
       " ('tito', 1),\n",
       " ('maniscalco', 1),\n",
       " ('screw', 1),\n",
       " ('sweet', 1),\n",
       " ('soda', 1),\n",
       " ('example', 1),\n",
       " ('clit', 1),\n",
       " ('fell', 1),\n",
       " ('adult', 1),\n",
       " ('hamilton', 1),\n",
       " ('push', 1),\n",
       " ('quick', 1),\n",
       " ('dinner', 1),\n",
       " ('claw', 1),\n",
       " ('genius', 1),\n",
       " ('yet', 1),\n",
       " ('wellplayed', 1),\n",
       " ('fear', 1),\n",
       " ('mine', 1),\n",
       " ('marriage', 1),\n",
       " ('figure', 1),\n",
       " ('mind', 1),\n",
       " ('hot', 1),\n",
       " ('seem', 1),\n",
       " ('owner', 1),\n",
       " ('low', 1),\n",
       " ('mini', 1),\n",
       " ('daphne', 1),\n",
       " ('charlie', 1),\n",
       " ('amazon', 1),\n",
       " ('bolsonaro', 1),\n",
       " ('brazil', 1),\n",
       " ('indigenous', 1),\n",
       " ('rainforest', 1),\n",
       " ('sônia', 1),\n",
       " ('deforestation', 1),\n",
       " ('guajajara', 1),\n",
       " ('jbs', 1),\n",
       " ('bolsonaros', 1),\n",
       " ('minhaj', 1),\n",
       " ('jair', 1),\n",
       " ('meme', 1),\n",
       " ('batista', 1),\n",
       " ('brazilian', 1),\n",
       " ('million', 1),\n",
       " ('earth', 1),\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate this list and identify the most common words along with how many transcripts they occur in\n",
    "most_common_words = Counter(words).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "053ec2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our own stop word list based on top words \n",
    "# we consider the word as a stop word if >= 150 transcript have it as top word\n",
    "\n",
    "add_stop_words = [word for word, count in most_common_words if count >= 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bc6bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle\n",
    "with open('pickle/' + 'mostcommonwords-st.pkl', 'wb') as f:\n",
    "    pickle.dump(most_common_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0d32550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['get',\n",
       " 'go',\n",
       " 'know',\n",
       " 'dont',\n",
       " 'im',\n",
       " 'like',\n",
       " 'say',\n",
       " 'thats',\n",
       " 'one',\n",
       " 'come',\n",
       " 'right',\n",
       " 'think',\n",
       " 'youre',\n",
       " 'people',\n",
       " 'see',\n",
       " 'look',\n",
       " 'want',\n",
       " 'time',\n",
       " 'make',\n",
       " 'na',\n",
       " 'gon',\n",
       " 'thing',\n",
       " 'oh',\n",
       " 'take',\n",
       " 'good',\n",
       " 'guy',\n",
       " 'fuck',\n",
       " 'would',\n",
       " 'yeah',\n",
       " 'tell',\n",
       " 'well',\n",
       " 'he',\n",
       " 'shit',\n",
       " 'cause',\n",
       " 'back',\n",
       " 'theyre',\n",
       " 'man',\n",
       " 'really',\n",
       " 'cant',\n",
       " 'little',\n",
       " 'let',\n",
       " 'just',\n",
       " 'okay',\n",
       " 'ive',\n",
       " '♪',\n",
       " '–',\n",
       " 'ta',\n",
       " 'uh',\n",
       " 'wan',\n",
       " 'g',\n",
       " 'e',\n",
       " 'ah',\n",
       " 'r',\n",
       " 'mi',\n",
       " 'le']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after a few iterations of checking the top words with Count Vectorizer\n",
    "# we created a list of stop words that needs to be removed too\n",
    "\n",
    "own_stop_words = ['just', 'okay', 'ive', '♪', '–', 'ta', 'uh', 'wan', 'g', 'e', 'ah', 'r', 'mi', 'le']\n",
    "complete_stop_words = [*add_stop_words, *own_stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80072992",
   "metadata": {},
   "source": [
    "## Helper Functions \n",
    "From 2-Data-Cleaning.ipynb file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f6d5bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same function as 2-Data-Cleaning \n",
    "def get_wordnet_pos(treebank_tag) : \n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # As default pos in lemmatization is Noun\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b65919d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same function as 2-Data-Cleaning \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def pos_then_lemmatize(pos_tagged_words) :\n",
    "    res = []\n",
    "    for pos in pos_tagged_words : \n",
    "        word = pos[0]\n",
    "        pos_tag = pos[1]\n",
    "\n",
    "        lem = lemmatizer.lemmatize(word, get_wordnet_pos(pos_tag))\n",
    "        res.append(lem)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd4e90af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new function \n",
    "def custom_tokenizer_stop(doc) : \n",
    "    words = word_tokenize(doc.lower())\n",
    "    \n",
    "    # add our own stop word list to the existing English stop words \n",
    "    new_stop_words = text.ENGLISH_STOP_WORDS.union(complete_stop_words)\n",
    "    \n",
    "    filtered_words = [w for w in words if not w in new_stop_words] \n",
    "    pos_tagged_words = nltk.pos_tag(filtered_words)\n",
    "    pos_lemmatized_words = pos_then_lemmatize(pos_tagged_words)\n",
    "    filtered_words_2 = [w for w in pos_lemmatized_words if not w in new_stop_words] \n",
    "    \n",
    "    return filtered_words_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a757c2",
   "metadata": {},
   "source": [
    "## An updated Document-Term Matrix \n",
    "\n",
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c39e9e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comedian</th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Subtitle</th>\n",
       "      <th>Transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chris Rock</td>\n",
       "      <td>March 8, 2023</td>\n",
       "      <td>Selective Outrage (2023) | Transcript</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lets go    she said  ill do anything you w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marc Maron</td>\n",
       "      <td>March 3, 2023</td>\n",
       "      <td>Thinky Pain (2013) | Transcript</td>\n",
       "      <td>Marc Maron returns to his old stomping grounds...</td>\n",
       "      <td>i dont know what you were thinking like im no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chelsea Handler</td>\n",
       "      <td>March 3, 2023</td>\n",
       "      <td>Evolution (2020) | Transcript</td>\n",
       "      <td>Chelsea Handler is back and better than ever -...</td>\n",
       "      <td>join me in welcoming the author of six number ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tom Papa</td>\n",
       "      <td>March 3, 2023</td>\n",
       "      <td>What A Day! (2022) | Transcript</td>\n",
       "      <td>Follows Papa as he shares about parenting, his...</td>\n",
       "      <td>premiered on december   ladies and gentlemen g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jim Jefferies</td>\n",
       "      <td>February 22, 2023</td>\n",
       "      <td>High n’ Dry (2023) | Transcript</td>\n",
       "      <td>Jim Jefferies is back and no topic is off limi...</td>\n",
       "      <td>please welcome to the stage jim jefferies hell...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Comedian               Date                                  Title  \\\n",
       "0       Chris Rock      March 8, 2023  Selective Outrage (2023) | Transcript   \n",
       "1       Marc Maron      March 3, 2023        Thinky Pain (2013) | Transcript   \n",
       "2  Chelsea Handler      March 3, 2023          Evolution (2020) | Transcript   \n",
       "3         Tom Papa      March 3, 2023        What A Day! (2022) | Transcript   \n",
       "4    Jim Jefferies  February 22, 2023        High n’ Dry (2023) | Transcript   \n",
       "\n",
       "                                            Subtitle  \\\n",
       "0                                                NaN   \n",
       "1  Marc Maron returns to his old stomping grounds...   \n",
       "2  Chelsea Handler is back and better than ever -...   \n",
       "3  Follows Papa as he shares about parenting, his...   \n",
       "4  Jim Jefferies is back and no topic is off limi...   \n",
       "\n",
       "                                          Transcript  \n",
       "0      lets go    she said  ill do anything you w...  \n",
       "1   i dont know what you were thinking like im no...  \n",
       "2  join me in welcoming the author of six number ...  \n",
       "3  premiered on december   ladies and gentlemen g...  \n",
       "4  please welcome to the stage jim jefferies hell...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the clean data \n",
    "df_clean = pd.read_pickle('/Users/lihuicham/Desktop/Y2S2/BT4222/project/standup-comedy-analysis/main/pickle/corpus.pkl')\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39032821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# (1, 2) : include bigram \n",
    "# max_features = 300 : choose features/words that occur most frequently to be its vocabulary \n",
    "cv = CountVectorizer(ngram_range = (1, 1),\n",
    "                    tokenizer = custom_tokenizer_stop)\n",
    "cv_vectors = cv.fit_transform(df_clean['Transcript'])\n",
    "cv_feature_names = cv.get_feature_names_out()\n",
    "cv_matrix_stop = pd.DataFrame(cv_vectors.toarray(), columns=cv_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e397c5",
   "metadata": {},
   "source": [
    "### Double Checking\n",
    "\n",
    "In the below code chunk, we double check whether our `completed_stop_words` list is working.  \n",
    "\n",
    "In `top_dict_check`, we can clearly see that now the words are starting to makes sense and are indeed meaningful in each transcript. The common top words that are meaningless and filler words are removed successfully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab662602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we double check on the top words in each transcript now. \n",
    "cv_matrix_check = cv_matrix_stop.transpose()\n",
    "\n",
    "top_dict_check_cv = {}\n",
    "for c in cv_matrix_check.columns:\n",
    "    top = cv_matrix_check[c].sort_values(ascending=False).head(30)\n",
    "    top_dict_check_cv[c]= list(zip(top.index, top.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d15af579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle \n",
    "with open('pickle/' + 'common_words_cv.pkl', 'wb') as f:\n",
    "    pickle.dump(top_dict_check_cv, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffbcff97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kid', 33),\n",
       " ('black', 33),\n",
       " ('woman', 31),\n",
       " ('try', 29),\n",
       " ('everybody', 26),\n",
       " ('school', 26),\n",
       " ('white', 25),\n",
       " ('love', 25),\n",
       " ('motherfucker', 23),\n",
       " ('ngga', 23),\n",
       " ('need', 22),\n",
       " ('talk', 21),\n",
       " ('lola', 21),\n",
       " ('year', 20),\n",
       " ('pussy', 19),\n",
       " ('day', 18),\n",
       " ('work', 18),\n",
       " ('shoe', 18),\n",
       " ('aint', 17),\n",
       " ('child', 16),\n",
       " ('girl', 16),\n",
       " ('lawyer', 16),\n",
       " ('didnt', 16),\n",
       " ('men', 15),\n",
       " ('mother', 15),\n",
       " ('baby', 15),\n",
       " ('accept', 14),\n",
       " ('attention', 14),\n",
       " ('sell', 12),\n",
       " ('bitch', 11)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we check with the first transcript \n",
    "first_transcript_value_cv = list(top_dict_check_cv.values())[0]\n",
    "first_transcript_value_cv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5539f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the updated document-term matrix from Count Vectorizer\n",
    "with open('pickle/' + 'cv_stop.pkl', 'wb') as f:\n",
    "    pickle.dump(cv_matrix_stop, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788f7047",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "We do the same for TF-IDF too.  \n",
    "Output : An updated TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01c6f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorizer\n",
    "\n",
    "tf = TfidfVectorizer(ngram_range = (1, 1),\n",
    "                    tokenizer = custom_tokenizer_stop)\n",
    "tf_vectors = tf.fit_transform(df_clean['Transcript'])\n",
    "tf_feature_names = tf.get_feature_names_out()\n",
    "tfidf_matrix_stop = pd.DataFrame(tf_vectors.toarray(), columns=tf_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cca09175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we double check on the top words in each transcript now. \n",
    "tf_matrix_check = tfidf_matrix_stop.transpose()\n",
    "\n",
    "top_dict_check_tf = {}\n",
    "for c in tf_matrix_check.columns:\n",
    "    top = tf_matrix_check[c].sort_values(ascending=False).head(30)\n",
    "    top_dict_check_tf[c]= list(zip(top.index, top.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "99eaed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle \n",
    "with open('pickle/' + 'common_words_tfidf.pkl', 'wb') as f:\n",
    "    pickle.dump(top_dict_check_tf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7847de1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lola', 0.36753783017238584),\n",
       " ('ngga', 0.3034142012409467),\n",
       " ('lawyer', 0.14154347913241483),\n",
       " ('black', 0.1389059795692906),\n",
       " ('motherfucker', 0.13089440529085258),\n",
       " ('kid', 0.11902042541909003),\n",
       " ('nggas', 0.11533644274132217),\n",
       " ('oj', 0.11446768159232128),\n",
       " ('pussy', 0.11238470741225165),\n",
       " ('woman', 0.10993135770144177),\n",
       " ('school', 0.10916810655918228),\n",
       " ('accept', 0.10699290692932405),\n",
       " ('prochoice', 0.10684845293716282),\n",
       " ('touché', 0.10684845293716282),\n",
       " ('everybody', 0.10622262315393302),\n",
       " ('shoe', 0.10273941410638181),\n",
       " ('white', 0.10138166293883931),\n",
       " ('try', 0.09847239157383815),\n",
       " ('kardashian', 0.09768550841638349),\n",
       " ('yoga', 0.0970973211812972),\n",
       " ('attention', 0.09409720481079092),\n",
       " ('aint', 0.09266061803079417),\n",
       " ('draymond', 0.09111095879801154),\n",
       " ('inlaws', 0.08973820811704201),\n",
       " ('abortion', 0.08892078503780722),\n",
       " ('love', 0.08591815048902417),\n",
       " ('victim', 0.08531639251172383),\n",
       " ('trimester', 0.08207771997589927),\n",
       " ('spoil', 0.08056287095631871),\n",
       " ('elon', 0.079922275140655)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_transcript_value_tf = list(top_dict_check_tf.values())[0]\n",
    "first_transcript_value_tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "194deb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickle/' + 'tfidf_stop.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_matrix_stop, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67007dcd",
   "metadata": {},
   "source": [
    "## Decision Making \n",
    "\n",
    "Now, we need to decide which document-term matrix to use for the project.  \n",
    "1. Count Vectorizer \n",
    "2. TF-IDF Vectorizer \n",
    "\n",
    "From the top words shown, **TF-IDF** might be a better matrix.  \n",
    "\n",
    "Reasons : \n",
    "* More meaningful words that are useful for topic modelling and EDA. For example, important nouns such as `'kardashian'`, `'trimester'` and `'victim'` are valued in TF-IDF matrix compared to Count Vectorizer matrix. These words are important for topic modelling. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
