{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b24f9487",
   "metadata": {},
   "source": [
    "# Topic Modeling with Empath \n",
    "\n",
    "## Empath module information \n",
    "GitHub Repo : [Empath](https://github.com/Ejhfast/empath-client)  \n",
    "Research publication : [Empath : Understanding Topic Signals in Large-Scale Text](https://arxiv.org/pdf/1602.06979.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a982fcf",
   "metadata": {},
   "source": [
    "We will compare three methods used for Topic Modeling \n",
    "1. **Document-Term Matrix** from TF-IDF Vectorizer  \n",
    "    Text is preprocessed (no stop words, lemmatization, parts-of-speech tagging and etc.) \n",
    "    Bag-of-Words where order of words is not preserved   \n",
    "\n",
    "2. **Corpus** (aka. full transcript)  \n",
    "   Order of words is preserved  \n",
    "\n",
    "3. **Subtitle**  \n",
    "   The original subtitle of each transcript, typically consists of one or two lines of summary of the transcript. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ceeeb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from empath import Empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26323527",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = Empath()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bb3344",
   "metadata": {},
   "source": [
    "### 1. Topic Modeling with Document-Term Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "646e3f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the top 30 common words dictionary created using TF-IDF vectorizer\n",
    "common_words_dict = pd.read_pickle('/Users/lihuicham/Desktop/Y2S2/BT4222/project/standup-comedy-analysis/main/pickle/common_words_tfidf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "077a6584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'woman ali cum cheat husband radiologist pussy dick men fan ct chill colonoscopy barbara orgasm magician standup wong power dude face single respect underwear millionaire year quality money pubes earn'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "for i in common_words_dict[32] : \n",
    "    words.append(i[0])\n",
    "    \n",
    "sent = ' '.join(words)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d2502456",
   "metadata": {},
   "outputs": [],
   "source": [
    "lext_dict_dtm = lexicon.analyze(sent, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "74eb3a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 15 categories \n",
    "dtm_topics = sorted(lext_dict_dtm, key=lext_dict_dtm.get, reverse=True)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbaf59f",
   "metadata": {},
   "source": [
    "### 2. Topic Modeling with Corpus (Full Transcript) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b79405c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comedian</th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Subtitle</th>\n",
       "      <th>Transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chris Rock</td>\n",
       "      <td>March 8, 2023</td>\n",
       "      <td>Selective Outrage (2023) | Transcript</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lets go    she said  ill do anything you w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marc Maron</td>\n",
       "      <td>March 3, 2023</td>\n",
       "      <td>Thinky Pain (2013) | Transcript</td>\n",
       "      <td>Marc Maron returns to his old stomping grounds...</td>\n",
       "      <td>i dont know what you were thinking like im no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chelsea Handler</td>\n",
       "      <td>March 3, 2023</td>\n",
       "      <td>Evolution (2020) | Transcript</td>\n",
       "      <td>Chelsea Handler is back and better than ever -...</td>\n",
       "      <td>join me in welcoming the author of six number ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tom Papa</td>\n",
       "      <td>March 3, 2023</td>\n",
       "      <td>What A Day! (2022) | Transcript</td>\n",
       "      <td>Follows Papa as he shares about parenting, his...</td>\n",
       "      <td>premiered on december   ladies and gentlemen g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jim Jefferies</td>\n",
       "      <td>February 22, 2023</td>\n",
       "      <td>High n’ Dry (2023) | Transcript</td>\n",
       "      <td>Jim Jefferies is back and no topic is off limi...</td>\n",
       "      <td>please welcome to the stage jim jefferies hell...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Comedian               Date                                  Title  \\\n",
       "0       Chris Rock      March 8, 2023  Selective Outrage (2023) | Transcript   \n",
       "1       Marc Maron      March 3, 2023        Thinky Pain (2013) | Transcript   \n",
       "2  Chelsea Handler      March 3, 2023          Evolution (2020) | Transcript   \n",
       "3         Tom Papa      March 3, 2023        What A Day! (2022) | Transcript   \n",
       "4    Jim Jefferies  February 22, 2023        High n’ Dry (2023) | Transcript   \n",
       "\n",
       "                                            Subtitle  \\\n",
       "0                                                NaN   \n",
       "1  Marc Maron returns to his old stomping grounds...   \n",
       "2  Chelsea Handler is back and better than ever -...   \n",
       "3  Follows Papa as he shares about parenting, his...   \n",
       "4  Jim Jefferies is back and no topic is off limi...   \n",
       "\n",
       "                                          Transcript  \n",
       "0      lets go    she said  ill do anything you w...  \n",
       "1   i dont know what you were thinking like im no...  \n",
       "2  join me in welcoming the author of six number ...  \n",
       "3  premiered on december   ladies and gentlemen g...  \n",
       "4  please welcome to the stage jim jefferies hell...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load corpus \n",
    "df = pd.read_pickle('/Users/lihuicham/Desktop/Y2S2/BT4222/project/standup-comedy-analysis/main/pickle/corpus.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "eda2babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = df.Transcript[32].strip()\n",
    "# transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b86910bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lext_dict_transcript = lexicon.analyze(transcript, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "759117ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 20 categories \n",
    "corpus_topics = sorted(lext_dict_transcript, key=lext_dict_transcript.get, reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1513cbc",
   "metadata": {},
   "source": [
    "### 3. Topic Modeling with Subtitle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ea437477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ali Wong discusses her deepest fantasies, the challenges of monogamy, and her feelings about single people.'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtitle = df.Subtitle[32]\n",
    "subtitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7f3fee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lext_dict_subtitle = lexicon.analyze(subtitle, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0acb5723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 categories \n",
    "subtitle_topics = sorted(lext_dict_subtitle, key=lext_dict_subtitle.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4144ab",
   "metadata": {},
   "source": [
    "## Comparing the three methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5fa67aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['children',\n",
       " 'family',\n",
       " 'speaking',\n",
       " 'swearing_terms',\n",
       " 'body',\n",
       " 'communication',\n",
       " 'positive_emotion',\n",
       " 'negative_emotion',\n",
       " 'wedding',\n",
       " 'giving',\n",
       " 'sexual',\n",
       " 'business',\n",
       " 'masculine',\n",
       " 'feminine',\n",
       " 'home',\n",
       " 'party',\n",
       " 'trust',\n",
       " 'appearance',\n",
       " 'valuable',\n",
       " 'friends']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_topics  # 20 topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d2ff0c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['power',\n",
       " 'money',\n",
       " 'magic',\n",
       " 'banking',\n",
       " 'sexual',\n",
       " 'valuable',\n",
       " 'children',\n",
       " 'wedding',\n",
       " 'cold',\n",
       " 'family',\n",
       " 'masculine',\n",
       " 'wealthy',\n",
       " 'social_media',\n",
       " 'fear',\n",
       " 'body']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_topics  # 15 topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2e8b5440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['help',\n",
       " 'office',\n",
       " 'dance',\n",
       " 'money',\n",
       " 'wedding',\n",
       " 'domestic_work',\n",
       " 'sleep',\n",
       " 'medical_emergency',\n",
       " 'cold',\n",
       " 'hate']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtitle_topics  # 10 topics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6299ed",
   "metadata": {},
   "source": [
    "### Getting the common topics by pairing the methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "03ccb82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['children', 'family', 'body', 'wedding', 'sexual', 'masculine', 'valuable']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corpus X dtm \n",
    "set_corpus_dtm = set(corpus_topics)&set(dtm_topics) \n",
    "corpus_dtm = sorted(set_corpus_dtm, key = lambda k : corpus_topics.index(k))\n",
    "corpus_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2e572db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['money', 'wedding', 'cold']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dtm X subtitle \n",
    "set_dtm_subtitle = set(dtm_topics)&set(subtitle_topics) \n",
    "dtm_subtitle = sorted(set_dtm_subtitle, key = lambda k : dtm_topics.index(k))\n",
    "dtm_subtitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "af2ec253",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wedding']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corpus X subtitle  \n",
    "set_corpus_subtitle = set(corpus_topics)&set(subtitle_topics) \n",
    "corpus_subtitle = sorted(set_corpus_subtitle, key = lambda k : corpus_topics.index(k))\n",
    "corpus_subtitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2ea9787b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wedding']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corpus X dtm X subtitle \n",
    "set_all_methods = set(corpus_topics)&set(subtitle_topics)&set(dtm_topics) \n",
    "all_methods = sorted(set_corpus_subtitle, key = lambda k : corpus_topics.index(k))\n",
    "all_methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db6f442",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "We can consider combining all three methods and produce one or two most possible topic(s) mentioned in each transcript. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9be266c",
   "metadata": {},
   "source": [
    "## Test \n",
    "Use one simple sentence to test with the `Empath` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "851f15ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_dict_test = lexicon.analyze(\"he hit the other person\", normalize=True)\n",
    "# uncomment below line to see the normalized counts of each categories \n",
    "# lex_dict_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ef2927ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movement', 'violence', 'pain', 'negative_emotion', 'help']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the top 5 categories \n",
    "sorted(lex_dict_test, key=lex_dict_test.get, reverse=True)[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
