{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34385a18",
   "metadata": {},
   "source": [
    "# More Data Cleaning \n",
    "\n",
    "We realised that the document-term matrices we created in 2-Data-Cleaning.ipynb using Count Vectorizer and TF-IDF Vectorizer has a lot of meaningless filler words and common words such as `'like'`, `'just'`, `'people'`, `'youre'` and etc. \n",
    "\n",
    "Therefore, we wish to inspect the matrices further and create a new stop words list in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66002881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import re\n",
    "import string \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f50f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the document-term matrix formed by Count Vectorizer \n",
    "df_cv = pd.read_pickle('/Users/lihuicham/Desktop/Y2S2/BT4222/project/standup-comedy-analysis/main/pickle/cv.pkl')\n",
    "# transpose to term-document matrix \n",
    "df_cv = df_cv.transpose()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9241b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 50 words in each transcript \n",
    "top_dict = {}\n",
    "for c in df_cv.columns:\n",
    "    top = df_cv[c].sort_values(ascending=False).head(50)\n",
    "    top_dict[c]= list(zip(top.index, top.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21c7a0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 50 words in each transcript \n",
    "# for transcript, top_words in top_dict.items():\n",
    "#     print(transcript)\n",
    "#     print(', '.join([word for word, count in top_words[0:49]]))\n",
    "#     print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac40ff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add the most common top words to a stop word list\n",
    "\n",
    "# Let's first pull out the top 50 words for each comedian\n",
    "words = []\n",
    "for transcript in df_cv.columns:\n",
    "    top = [word for (word, count) in top_dict[transcript]]\n",
    "    for t in top:\n",
    "        words.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0747191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate this list and identify the most common words along with how many transcripts they occur in\n",
    "most_common_words = Counter(words).most_common()\n",
    "# most_common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "053ec2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our own stop word list based on top words \n",
    "# we consider the word as a stop word if >= 150 transcript have it as top word\n",
    "\n",
    "add_stop_words = [word for word, count in most_common_words if count >= 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0d32550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['get',\n",
       " 'go',\n",
       " 'know',\n",
       " 'dont',\n",
       " 'im',\n",
       " 'like',\n",
       " 'say',\n",
       " 'thats',\n",
       " 'one',\n",
       " 'come',\n",
       " 'right',\n",
       " 'think',\n",
       " 'youre',\n",
       " 'people',\n",
       " 'see',\n",
       " 'look',\n",
       " 'want',\n",
       " 'time',\n",
       " 'make',\n",
       " 'na',\n",
       " 'gon',\n",
       " 'thing',\n",
       " 'oh',\n",
       " 'take',\n",
       " 'good',\n",
       " 'guy',\n",
       " 'fuck',\n",
       " 'would',\n",
       " 'yeah',\n",
       " 'tell',\n",
       " 'well',\n",
       " 'he',\n",
       " 'shit',\n",
       " 'cause',\n",
       " 'back',\n",
       " 'theyre',\n",
       " 'man',\n",
       " 'really',\n",
       " 'cant',\n",
       " 'little',\n",
       " 'let',\n",
       " 'just',\n",
       " 'okay',\n",
       " 'ive',\n",
       " '♪',\n",
       " '–',\n",
       " 'ta',\n",
       " 'uh',\n",
       " 'wan',\n",
       " 'g',\n",
       " 'e',\n",
       " 'ah',\n",
       " 'r',\n",
       " 'mi',\n",
       " 'le']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after a few iterations of checking the top words with Count Vectorizer\n",
    "# we created a list of stop words that needs to be removed too\n",
    "\n",
    "own_stop_words = ['just', 'okay', 'ive', '♪', '–', 'ta', 'uh', 'wan', 'g', 'e', 'ah', 'r', 'mi', 'le']\n",
    "complete_stop_words = [*add_stop_words, *own_stop_words]\n",
    "complete_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80072992",
   "metadata": {},
   "source": [
    "## Helper Functions \n",
    "From 2-Data-Cleaning.ipynb file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f6d5bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same function as 2-Data-Cleaning \n",
    "def get_wordnet_pos(treebank_tag) : \n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # As default pos in lemmatization is Noun\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b65919d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same function as 2-Data-Cleaning \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def pos_then_lemmatize(pos_tagged_words) :\n",
    "    res = []\n",
    "    for pos in pos_tagged_words : \n",
    "        word = pos[0]\n",
    "        pos_tag = pos[1]\n",
    "\n",
    "        lem = lemmatizer.lemmatize(word, get_wordnet_pos(pos_tag))\n",
    "        res.append(lem)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd4e90af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new function \n",
    "def custom_tokenizer_stop(doc) : \n",
    "    words = word_tokenize(doc.lower())\n",
    "    \n",
    "    # add our own stop word list to the existing English stop words \n",
    "    new_stop_words = text.ENGLISH_STOP_WORDS.union(complete_stop_words)\n",
    "    \n",
    "    filtered_words = [w for w in words if not w in new_stop_words] \n",
    "    pos_tagged_words = nltk.pos_tag(filtered_words)\n",
    "    pos_lemmatized_words = pos_then_lemmatize(pos_tagged_words)\n",
    "    filtered_words_2 = [w for w in pos_lemmatized_words if not w in new_stop_words] \n",
    "    \n",
    "    return filtered_words_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a757c2",
   "metadata": {},
   "source": [
    "## An updated Document-Term Matrix \n",
    "\n",
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c39e9e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comedian</th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Subtitle</th>\n",
       "      <th>Transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chris Rock</td>\n",
       "      <td>March 8, 2023</td>\n",
       "      <td>Selective Outrage (2023) | Transcript</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lets go    she said  ill do anything you w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marc Maron</td>\n",
       "      <td>March 3, 2023</td>\n",
       "      <td>Thinky Pain (2013) | Transcript</td>\n",
       "      <td>Marc Maron returns to his old stomping grounds...</td>\n",
       "      <td>i dont know what you were thinking like im no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chelsea Handler</td>\n",
       "      <td>March 3, 2023</td>\n",
       "      <td>Evolution (2020) | Transcript</td>\n",
       "      <td>Chelsea Handler is back and better than ever -...</td>\n",
       "      <td>join me in welcoming the author of six number ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tom Papa</td>\n",
       "      <td>March 3, 2023</td>\n",
       "      <td>What A Day! (2022) | Transcript</td>\n",
       "      <td>Follows Papa as he shares about parenting, his...</td>\n",
       "      <td>premiered on december   ladies and gentlemen g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jim Jefferies</td>\n",
       "      <td>February 22, 2023</td>\n",
       "      <td>High n’ Dry (2023) | Transcript</td>\n",
       "      <td>Jim Jefferies is back and no topic is off limi...</td>\n",
       "      <td>please welcome to the stage jim jefferies hell...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Comedian               Date                                  Title  \\\n",
       "0       Chris Rock      March 8, 2023  Selective Outrage (2023) | Transcript   \n",
       "1       Marc Maron      March 3, 2023        Thinky Pain (2013) | Transcript   \n",
       "2  Chelsea Handler      March 3, 2023          Evolution (2020) | Transcript   \n",
       "3         Tom Papa      March 3, 2023        What A Day! (2022) | Transcript   \n",
       "4    Jim Jefferies  February 22, 2023        High n’ Dry (2023) | Transcript   \n",
       "\n",
       "                                            Subtitle  \\\n",
       "0                                                NaN   \n",
       "1  Marc Maron returns to his old stomping grounds...   \n",
       "2  Chelsea Handler is back and better than ever -...   \n",
       "3  Follows Papa as he shares about parenting, his...   \n",
       "4  Jim Jefferies is back and no topic is off limi...   \n",
       "\n",
       "                                          Transcript  \n",
       "0      lets go    she said  ill do anything you w...  \n",
       "1   i dont know what you were thinking like im no...  \n",
       "2  join me in welcoming the author of six number ...  \n",
       "3  premiered on december   ladies and gentlemen g...  \n",
       "4  please welcome to the stage jim jefferies hell...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the clean data \n",
    "df_clean = pd.read_pickle('/Users/lihuicham/Desktop/Y2S2/BT4222/project/standup-comedy-analysis/main/pickle/corpus.pkl')\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39032821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# (1, 2) : include bigram \n",
    "# max_features = 300 : choose features/words that occur most frequently to be its vocabulary \n",
    "cv = CountVectorizer(ngram_range = (1, 1),\n",
    "                    tokenizer = custom_tokenizer_stop)\n",
    "cv_vectors = cv.fit_transform(df_clean['Transcript'])\n",
    "cv_feature_names = cv.get_feature_names_out()\n",
    "cv_matrix_stop = pd.DataFrame(cv_vectors.toarray(), columns=cv_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e397c5",
   "metadata": {},
   "source": [
    "### Double Checking\n",
    "\n",
    "In the below code chunk, we double check whether our `completed_stop_words` list is working.  \n",
    "\n",
    "In `top_dict_check`, we can clearly see that now the words are starting to makes sense and are indeed meaningful in each transcript. The common top words that are meaningless and filler words are removed successfully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab662602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we double check on the top words in each transcript now. \n",
    "cv_matrix_check = cv_matrix_stop.transpose()\n",
    "\n",
    "top_dict_check_cv = {}\n",
    "for c in cv_matrix_check.columns:\n",
    "    top = cv_matrix_check[c].sort_values(ascending=False).head(30)\n",
    "    top_dict_check_cv[c]= list(zip(top.index, top.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7b680b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kid', 33),\n",
       " ('black', 33),\n",
       " ('woman', 31),\n",
       " ('try', 29),\n",
       " ('everybody', 26),\n",
       " ('school', 26),\n",
       " ('white', 25),\n",
       " ('love', 25),\n",
       " ('motherfucker', 23),\n",
       " ('ngga', 23),\n",
       " ('need', 22),\n",
       " ('talk', 21),\n",
       " ('lola', 21),\n",
       " ('year', 20),\n",
       " ('pussy', 19),\n",
       " ('day', 18),\n",
       " ('work', 18),\n",
       " ('shoe', 18),\n",
       " ('aint', 17),\n",
       " ('child', 16),\n",
       " ('girl', 16),\n",
       " ('lawyer', 16),\n",
       " ('didnt', 16),\n",
       " ('men', 15),\n",
       " ('mother', 15),\n",
       " ('baby', 15),\n",
       " ('accept', 14),\n",
       " ('attention', 14),\n",
       " ('sell', 12),\n",
       " ('bitch', 11)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we check with the first transcript \n",
    "first_transcript_value_cv = list(top_dict_check_cv.values())[0]\n",
    "first_transcript_value_cv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5539f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the updated document-term matrix from Count Vectorizer\n",
    "with open('pickle/' + 'cv_stop.pkl', 'wb') as f:\n",
    "    pickle.dump(cv_matrix_stop, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788f7047",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "We do the same for TF-IDF too.  \n",
    "Output : An updated TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01c6f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorizer\n",
    "\n",
    "tf = TfidfVectorizer(ngram_range = (1, 1),\n",
    "                    tokenizer = custom_tokenizer_stop)\n",
    "tf_vectors = tf.fit_transform(df_clean['Transcript'])\n",
    "tf_feature_names = tf.get_feature_names_out()\n",
    "tfidf_matrix_stop = pd.DataFrame(tf_vectors.toarray(), columns=tf_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cca09175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we double check on the top words in each transcript now. \n",
    "tf_matrix_check = tfidf_matrix_stop.transpose()\n",
    "\n",
    "top_dict_check_tf = {}\n",
    "for c in tf_matrix_check.columns:\n",
    "    top = tf_matrix_check[c].sort_values(ascending=False).head(30)\n",
    "    top_dict_check_tf[c]= list(zip(top.index, top.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af9fec0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lola', 0.36753783017238584),\n",
       " ('ngga', 0.3034142012409467),\n",
       " ('lawyer', 0.14154347913241483),\n",
       " ('black', 0.1389059795692906),\n",
       " ('motherfucker', 0.13089440529085258),\n",
       " ('kid', 0.11902042541909003),\n",
       " ('nggas', 0.11533644274132217),\n",
       " ('oj', 0.11446768159232128),\n",
       " ('pussy', 0.11238470741225165),\n",
       " ('woman', 0.10993135770144177),\n",
       " ('school', 0.10916810655918228),\n",
       " ('accept', 0.10699290692932405),\n",
       " ('prochoice', 0.10684845293716282),\n",
       " ('touché', 0.10684845293716282),\n",
       " ('everybody', 0.10622262315393302),\n",
       " ('shoe', 0.10273941410638181),\n",
       " ('white', 0.10138166293883931),\n",
       " ('try', 0.09847239157383815),\n",
       " ('kardashian', 0.09768550841638349),\n",
       " ('yoga', 0.0970973211812972),\n",
       " ('attention', 0.09409720481079092),\n",
       " ('aint', 0.09266061803079417),\n",
       " ('draymond', 0.09111095879801154),\n",
       " ('inlaws', 0.08973820811704201),\n",
       " ('abortion', 0.08892078503780722),\n",
       " ('love', 0.08591815048902417),\n",
       " ('victim', 0.08531639251172383),\n",
       " ('trimester', 0.08207771997589927),\n",
       " ('spoil', 0.08056287095631871),\n",
       " ('elon', 0.079922275140655)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_transcript_value_tf = list(top_dict_check_tf.values())[0]\n",
    "first_transcript_value_tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "194deb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the updated document-term matrix from TF-IDF \n",
    "with open('pickle/' + 'tfidf_stop.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_matrix_stop, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67007dcd",
   "metadata": {},
   "source": [
    "## Decision Making \n",
    "\n",
    "Now, we need to decide which document-term matrix to use for the project.  \n",
    "1. Count Vectorizer \n",
    "2. TF-IDF Vectorizer \n",
    "\n",
    "From the top words shown, **TF-IDF** might be a better matrix.  \n",
    "\n",
    "Reasons : \n",
    "* More meaningful words that are useful for topic modelling and EDA. For example, important nouns such as `'kardashian'`, `'trimester'` and `'victim'` are valued in TF-IDF matrix compared to Count Vectorizer matrix. These words are important for topic modelling. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
